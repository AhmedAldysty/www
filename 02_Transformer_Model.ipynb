{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvwPfSNOs6Sn"
      },
      "source": [
        "# Generate Modbus TCP packet Using Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URJDzgkLxsFL"
      },
      "source": [
        "##[Data Link](https://drive.google.com/file/d/1L3QYDOLdWSPRy6nCSM2-Dl_A9mDKkwU-/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmkcpoCwq4ty"
      },
      "source": [
        "### Connect To Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZIQ_HspCNhM",
        "outputId": "e37b4f8a-b9df-423b-ea3a-8261119fe30e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY5Jnp3zrfr8"
      },
      "source": [
        "### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2co23gZsXvXn",
        "outputId": "20970006-0c7a-48a2-fd74-8023a396f36e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.4.0.post0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJcEPqeSrk4n"
      },
      "source": [
        "###Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG9tw2C3KpbJ",
        "outputId": "2e044972-6d76-4830-f978-c831d4b7021d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import math\n",
        "import torchtext.datasets as datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torchmetrics\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEPnWbq6rr2X"
      },
      "source": [
        "### Read & Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IeSA1pu7KzPK",
        "outputId": "5ed04067-cc37-4955-a834-5f5bf3ab97d5"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/data_final.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4luFqNJZXjK"
      },
      "outputs": [],
      "source": [
        "#df = df[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DunlbCEdyj4b",
        "outputId": "b70fdcb9-b1fd-46f7-d2a9-8f6559af7a15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 609934 entries, 0 to 609933\n",
            "Data columns (total 19 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   0       609934 non-null  object\n",
            " 1   1       609934 non-null  object\n",
            " 2   2       609934 non-null  object\n",
            " 3   3       609934 non-null  object\n",
            " 4   4       609934 non-null  object\n",
            " 5   5       609934 non-null  object\n",
            " 6   6       609934 non-null  object\n",
            " 7   7       609934 non-null  object\n",
            " 8   8       609934 non-null  object\n",
            " 9   9       609934 non-null  object\n",
            " 10  10      609934 non-null  object\n",
            " 11  11      609934 non-null  object\n",
            " 12  12      152741 non-null  object\n",
            " 13  13      152741 non-null  object\n",
            " 14  14      76203 non-null   object\n",
            " 15  15      76203 non-null   object\n",
            " 16  16      76203 non-null   object\n",
            " 17  17      76203 non-null   object\n",
            " 18  18      76203 non-null   object\n",
            "dtypes: object(19)\n",
            "memory usage: 88.4+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIk9nwnhrxog"
      },
      "source": [
        "### Define Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i-iIqSUK5hu"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.df = df\n",
        "        self.sos_token = torch.tensor([256], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([257], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([258], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_target_pair = self.df[idx]\n",
        "\n",
        "        enc_input_tokens = src_target_pair[:8]\n",
        "        dec_input_tokens = src_target_pair[8:]\n",
        "\n",
        "        # Add sos, eos and padding to each sentence\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
        "        # We will only add <s>, and </s> only on the label\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "        # Add <s> and </s> token\n",
        "        encoder_input = torch.tensor(enc_input_tokens, dtype=torch.int64)\n",
        "\n",
        "        # Add only <s> token\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only </s> token\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Double check the size of the tensors to make sure they are all seq_len long\n",
        "        #assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,  # (seq_len)\n",
        "            \"decoder_input\": decoder_input,  # (seq_len)\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
        "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
        "            \"label\": label,  # (seq_len)\n",
        "        }\n",
        "\n",
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI6vrl8ar6gc"
      },
      "source": [
        "### Define Transformer model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBX-s5Pezomi"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=1N-GpbPpBJd4ZQ-YeK2C-bPLUvkrq4gpr' height=600px width=400px>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0vpKgzvPZSc"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
        "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, hidden_size)\n",
        "         # Keep the dimension for broadcasting\n",
        "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "        # Keep the dimension for broadcasting\n",
        "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "        # eps is to prevent dividing by zero or when std is very small\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
        "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Create a matrix of shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # Create a vector of shape (seq_len)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
        "        # Create a vector of shape (d_model)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
        "        # Apply sine to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
        "        # Apply cosine to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
        "        # Add a batch dimension to the positional encoding\n",
        "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "        # Register the positional encoding as a buffer\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "        def __init__(self, features: int, dropout: float) -> None:\n",
        "            super().__init__()\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.norm = LayerNormalization(features)\n",
        "\n",
        "        def forward(self, x, sublayer):\n",
        "            return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # Embedding vector size\n",
        "        self.h = h # Number of heads\n",
        "        # Make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        # return attention scores which can be used for visualization\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return self.proj(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        # (batch, seq_len, d_model)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        # (batch, seq_len, vocab_size)\n",
        "        return self.projection_layer(x)\n",
        "\n",
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmpbJsh1Qcln"
      },
      "source": [
        "### Define Load Dataset Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnyNZPG4Ti9M"
      },
      "outputs": [],
      "source": [
        "def hex2dec(hex_str):\n",
        "        return int(hex_str, 0)\n",
        "\n",
        "def compine_df_in_one_row(df):\n",
        "    df = df.applymap(lambda x: hex2dec(x) if isinstance(x, str) and x.startswith('0x') else x)\n",
        "    data = []\n",
        "    for i in range(len(df)):\n",
        "        row = []\n",
        "        for item in df.iloc[i, :]:\n",
        "            if item != -1:\n",
        "                row.append(item)\n",
        "\n",
        "        data.append(row)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHat1cb5PCbh"
      },
      "outputs": [],
      "source": [
        "def get_ds(df, seq_len, batch_size):\n",
        "    # Keep 90% for training, 10% for validation\n",
        "    df_full = df.fillna(-1)\n",
        "    df_full = compine_df_in_one_row(df_full)\n",
        "    train_ds_size = int(0.99 * len(df_full))\n",
        "    print(f\"train size = {train_ds_size}\")\n",
        "    val_ds_size = len(df_full) - int(0.99 * len(df_full))\n",
        "    train_df, val_df = random_split(df_full, [train_ds_size, val_ds_size])\n",
        "    #train_df = df_full\n",
        "    train_ds = CustomDataset(train_df, seq_len)\n",
        "    val_ds = CustomDataset(val_df, seq_len)\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, seq_len, seq_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5pdHzSyQIdT"
      },
      "source": [
        "### Transformer Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc4Cf_8ZQhmd"
      },
      "outputs": [],
      "source": [
        "def masked_accuracy(prediction, target, mask):\n",
        "    # Ensure prediction are the same shape as target and mask\n",
        "\n",
        "    if prediction.shape == target.shape:\n",
        "\n",
        "        # Apply the mask to filter out invalid elements\n",
        "        masked_prediction = prediction[mask == 1]\n",
        "        masked_target = target[mask == 1]\n",
        "\n",
        "        # Calculate accuracy\n",
        "        correct_prediction = (masked_prediction.cpu() == masked_target.cpu()).sum().item()\n",
        "        total_valid_elements = mask.sum().item()\n",
        "\n",
        "        accuracy = correct_prediction / total_valid_elements\n",
        "        return accuracy\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjmypYG3S-Ro"
      },
      "outputs": [],
      "source": [
        "def accuracy(predictions, targets, masks):\n",
        "    sum = 0\n",
        "    for prediction, target, mask in zip(predictions, targets, masks):\n",
        "        sum += masked_accuracy(prediction, target, mask)\n",
        "    return sum / len(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTUsYTw_tqTM"
      },
      "source": [
        "### Define Model Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL4V3Fw8RjSj"
      },
      "outputs": [],
      "source": [
        "def get_model(src_len, tgt_len, seq_len, d_model):\n",
        "    model = build_transformer(src_len, tgt_len, seq_len, seq_len, d_model=d_model)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBvwteufXTLb"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(model, source, source_mask, max_len, device):\n",
        "    sos_idx = 256\n",
        "    eos_idx = 257\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # build mask for target\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # calculate output\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "        # get next token\n",
        "        prob = model.project(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "        )\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDbebUd_XNLd"
      },
      "outputs": [],
      "source": [
        "def run_validation(model, validation_ds, max_len, device, print_msg, global_step, writer, training_accuracy, num_examples=5):\n",
        "    model.eval()\n",
        "    pad = 258\n",
        "    count = 0\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "    targets = []\n",
        "    predictions = []\n",
        "    masks = []\n",
        "    try:\n",
        "        # get the console window width\n",
        "        with os.popen('stty size', 'r') as console:\n",
        "            _, console_width = console.read().split()\n",
        "            console_width = int(console_width)\n",
        "    except:\n",
        "        # If we can't get the console width, use 80 as default\n",
        "        console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
        "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
        "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
        "\n",
        "            # Run the tensors through the encoder, decoder and the projection layer\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
        "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
        "            targets += batch['label']\n",
        "            predictions += proj_output.argmax(dim=2)\n",
        "            masks += batch['label'] != pad\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "            # check that the batch size is 1\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, max_len, device)\n",
        "            source = [str(int(x)) for x in batch[\"encoder_input\"][0]]\n",
        "            target = [str(int(x)) for x in batch[\"label\"][0]]\n",
        "            index = target.index(\"257\")\n",
        "            target = target[:index + 1]\n",
        "            model_out = [str(int(x)) for x in model_out.detach().cpu().numpy()]\n",
        "            model_out = model_out[1:]\n",
        "            source_text = \" \".join(source)\n",
        "            target_text = \" \".join(target)\n",
        "            model_out_text = \" \".join(model_out)\n",
        "\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            predicted.append(model_out_text)\n",
        "\n",
        "\n",
        "\n",
        "            if count <= num_examples:\n",
        "                # Print the source, target and model output\n",
        "                print_msg('-'*console_width)\n",
        "                print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "                print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "                print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "                print_msg('-'*console_width)\n",
        "\n",
        "    print(f'training accurcy: {training_accuracy}')\n",
        "    print(f'validation accurcy: {accuracy(predictions, targets, masks)}')\n",
        "    if writer:\n",
        "        # Evaluate the character error rate\n",
        "        # Compute the char error rate\n",
        "        metric = torchmetrics.CharErrorRate()\n",
        "        cer = metric(predicted, expected)\n",
        "        print(f\"CER: {cer:.3f}\")\n",
        "        writer.add_scalar('validation cer', cer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the word error rate\n",
        "        metric = torchmetrics.WordErrorRate()\n",
        "        wer = metric(predicted, expected)\n",
        "        print(f\"WER: {wer:.3f}\")\n",
        "        writer.add_scalar('validation wer', wer, global_step)\n",
        "        writer.flush()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62aQwgz4dzUc"
      },
      "outputs": [],
      "source": [
        "def masked_loss(loss_fn, outputs, labels, pad_token=258):\n",
        "    mask = (labels != pad_token).float()\n",
        "    loss = loss_fn(outputs.view(-1, outputs.size(-1)), labels.view(-1)) * mask.view(-1)\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhLhIrz7U5Qz"
      },
      "outputs": [],
      "source": [
        "def train_model(df, epochs, lr, seq_len, src_len, tgt_len, d_model, show_samples, batch_size=32):\n",
        "    # Define the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    if (device == 'cuda'):\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "    elif (device == 'mps'):\n",
        "        print(f\"Device name: <mps>\")\n",
        "    else:\n",
        "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
        "        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
        "        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
        "    device = torch.device(device)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(df, seq_len, batch_size)\n",
        "    model = get_model(src_len, tgt_len, seq_len, d_model).to(device)\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter('runs/train transformer')\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-9)\n",
        "\n",
        "    # If the user specified a model to preload before training, load it\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=258, label_smoothing=0.1).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, epochs):\n",
        "        pad = 258\n",
        "        targets = []\n",
        "        predictions = []\n",
        "        masks = []\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "\n",
        "        for batch in batch_iterator:\n",
        "\n",
        "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
        "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
        "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
        "\n",
        "            # Run the tensors through the encoder, decoder and the projection layer\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
        "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
        "            # Compare the output with the label\n",
        "            label = batch['label'].to(device) # (B, seq_len)\n",
        "            # Compute the loss using a simple cross entropy\n",
        "            loss = masked_loss(loss_fn, proj_output, label)\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            # Log the loss\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            global_step += 1\n",
        "            targets += batch['label']\n",
        "            predictions += proj_output.argmax(dim=2)\n",
        "            masks += batch['label'] != pad\n",
        "\n",
        "\n",
        "        # Run validation at the end of every epoch\n",
        "        training_accuracy = accuracy(predictions, targets, masks)\n",
        "\n",
        "        run_validation(model, val_dataloader, seq_len, device, lambda msg: batch_iterator.write(msg), global_step, writer, training_accuracy, show_samples)\n",
        "        os.makedirs(f\"/content/drive/MyDrive/transformers/training\", exist_ok=True)\n",
        "        model_filename = f\"/content/drive/MyDrive/transformers/training/model_at_epoch_{epoch+1}\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)\n",
        "    os.makedirs(f\"/content/drive/MyDrive/transformers\", exist_ok=True)\n",
        "    model_filename = f\"/content/drive/MyDrive/transformers/final_model\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'global_step': global_step\n",
        "    }, model_filename)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVUgIjNCt2t6"
      },
      "source": [
        "### Set Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B-cZNs-cz18"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "lr = 0.0001\n",
        "show_samples = 5 # Show samples of validation each epoch\n",
        "seq_len = 21\n",
        "src_len = 259\n",
        "tgt_len = 259\n",
        "d_model = 512\n",
        "batch_size = 32\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EjAz9KwgXvL"
      },
      "outputs": [],
      "source": [
        "#train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(df, seq_len, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC_71McYuJfF"
      },
      "source": [
        "### Train The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs1s4U_HcvmR"
      },
      "outputs": [],
      "source": [
        "train_model(df, epochs, lr, seq_len, src_len, tgt_len, d_model, show_samples, batch_size) # commented to doesn't override the cell output in presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB-tCh0yuNUe"
      },
      "source": [
        "### Load The Trainined Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-g13X-SjDDS",
        "outputId": "2570fa05-69a0-498b-9b55-908175a99fc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
        "model = get_model(src_len, tgt_len, seq_len, d_model).to(device)\n",
        "#model_filename = \"/content/drive/MyDrive/transformers/final_model\"\n",
        "\n",
        "model_filename = \"/content/drive/MyDrive/transformers/training/model_at_epoch_20\"\n",
        "state = torch.load(model_filename, map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state['model_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5K7EEH7uVAC"
      },
      "source": [
        "###Define Generate Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-nElmLO_4uQ"
      },
      "outputs": [],
      "source": [
        "def str_encoder(str_):\n",
        "    return [int(x, 16) for x in str_.split()]\n",
        "\n",
        "def str_decoder(str_):\n",
        "    return [hex(x) for x in str_]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2MJYXMYJ76M"
      },
      "outputs": [],
      "source": [
        "def check_packet_start(str_):\n",
        "    return len(str_.split()) == 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DHnwwb2H8nO"
      },
      "outputs": [],
      "source": [
        "def generate_modbus_tcp_packet(model, packet_start, max_len, device):\n",
        "    try:\n",
        "        if check_packet_start(packet_start):\n",
        "            packet_start = str_encoder(packet_start)\n",
        "            input = torch.tensor(packet_start).unsqueeze(0).to(device)\n",
        "            mask = torch.tensor([1]*8).to(device)\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                output = greedy_decode(model, input, mask, max_len, device)\n",
        "                output = str_decoder(output.detach().cpu().numpy()[1:-1])\n",
        "                return \" \".join(str_decoder(packet_start)) + \" \" + \" \".join(output)\n",
        "        else:\n",
        "            return f\"Packet start is not valid: {packet_start}, please enter packet start as 8 hexadecimal values separated with spaces.\"\n",
        "    except:\n",
        "        return f\"Packet start is not valid: {packet_start}, please enter packet start as 8 hexadecimal values separated with spaces.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV9Q6uh6u0tf"
      },
      "source": [
        "### Generate Modbus TCP Packet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwLhrCaPu6tI",
        "outputId": "b8aa5118-9321-45bb-937f-61684860dc4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0x0 0x1 0x0 0x0 0x0 0x6 0x2 0x6 0x0 0x24 0x0 0x0\n"
          ]
        }
      ],
      "source": [
        "packet_start = \"0x0 0x1 0x0 0x0 0x0 0x6 0x2 0x6\" #MBAP + FC\n",
        "print(generate_modbus_tcp_packet(model, packet_start, seq_len, device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSRZd-D0VBDi"
      },
      "outputs": [],
      "source": [
        "\" \".join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw-KsAw2SZ0o"
      },
      "outputs": [],
      "source": [
        "generated_packets = []\n",
        "for index, row in df.iloc[:20000,:8].iterrows():\n",
        "    packet_start = \" \".join(row)\n",
        "    generated_packet = generate_modbus_tcp_packet(model, packet_start, seq_len, device)\n",
        "    generated_packets.append(generated_packet)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/transformers/packets.txt\", \"w\") as f:\n",
        "    for packet in generated_packets:\n",
        "        f.write(packet + \"\\n\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
